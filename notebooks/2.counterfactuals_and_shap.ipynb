{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59239cbb-a912-499c-b0f1-2fb9742217cb",
   "metadata": {},
   "source": [
    "# Explaining Facial Expression Recognition\n",
    "## Notebook 2:  XAI for Affective Computing with Counterfactuals and SHAP (SoSe2023)\n",
    "\n",
    "In the first task in this notebook, you will attempt to generate Counterfactual Explanations Facial Expression Recognition using Facial Action Units.  In the second task, you will generate SHAP based feature attribution explations for an image based CNN using the SHAP implementation of the DeepLift algorithm.  For both parts, we will be using the same models and data as the previous notebook.  \n",
    "\n",
    "To use this notebook, please make sure to go step by step through each of the cells review the code and comments along the way.\n",
    "\n",
    "Make sure to read the Notebook 2 section of the **README** beforing starting this notebook for all installation instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc2944a-a7fb-4d96-b652-8a2fe4f432d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39040a80-0531-4da1-a98a-bd59f87d6cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295770a4-0087-4da4-80f6-15d5df6a18a9",
   "metadata": {},
   "source": [
    "##### Import necessary libraries\n",
    "\n",
    "(see README for necessary package installations if you receive a `module not found` error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b037e5c-d2f7-4c98-968b-e57231811d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "# import tensorflow for model loading\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# import sklearn for processing data and results\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from skimage import io\n",
    "\n",
    "# import model loading function\n",
    "from model import cnn_model, create_bn_replacment\n",
    "\n",
    "import utils\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa542afb-62f4-4e6e-a2eb-b0c5228f7a83",
   "metadata": {},
   "source": [
    "##### Some Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42c3576-d1b6-4895-bb42-cd665ce6c535",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 12\n",
    "IMG_HEIGHT = 128\n",
    "IMG_WIDTH = 128\n",
    "BATCH_SIZE = 80 # set to 80 to easily load all images using image generator in one call\n",
    "NUM_CLASSES = 8\n",
    "CLASS_LABELS = ['Neutral', 'Happy', 'Sad', 'Surprise', 'Fear', 'Disgust', 'Anger', 'Contempt']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf15372d-c8d6-4aed-976a-06248e900a7e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 1: Counterfactuals with Facial Action Units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b545cb16-552d-488b-8ce8-e9f761a1894f",
   "metadata": {},
   "source": [
    "### Load the Model and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109bc59c-0383-40f4-b9a7-6dd9be9f36cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Load the Random Decision Forest (RDF) Model\n",
    "\n",
    "Here we will load the pretrained random decision forest (RDF) trained on the facial action units (FAUs) that were extracted from the AffectNet dataset using [OpenFace](https://github.com/TadasBaltrusaitis/OpenFace).\n",
    "\n",
    "(This is the same model as our previous notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e544a7-f251-486a-a9f4-a366a08b9b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../models/affect_rdf.pkl', 'rb') as f:\n",
    "    fer_rdf_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76485416-6e43-469c-ae60-edbddee085b2",
   "metadata": {},
   "source": [
    "#### Load the data\n",
    "\n",
    "Next we will load the preextracted FAUs from a `csv` file created by OpenFace during FAU extraction from the AffectNet dataset.  We load the data into Pandas Dataframes, then convert the columns to numpy array for easier processing with scikit-learn.\n",
    "\n",
    "The numpy array `X_aus` contains FAUs from the 80 images available for explainations.  And `Y_aus_true` stores the ground truth labels, encoded as [one hot vectors](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/) for each set of FAUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c56b3d8-28e9-457c-9eef-275b793b288d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "train_csv = '../data/affectnet_aus/train_aus.csv'\n",
    "df_aus_train = pd.read_csv(train_csv)\n",
    "\n",
    "# Small dataset for explanations\n",
    "xai_csv = '../data/affectnet_aus/eval_aus.csv'\n",
    "df_aus_xai = pd.read_csv(xai_csv)\n",
    "\n",
    "# get only the columns storing action units from the dataframe\n",
    "feature_cols = [col for col in df_aus_xai if col.startswith('AU')]\n",
    "\n",
    "X_aus = np.array(df_aus_xai.loc[:, feature_cols])\n",
    "Y_aus_true = np.array(df_aus_xai['class'])\n",
    "\n",
    "print('XAI Dataset', X_aus.shape, Y_aus_true.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7e4945-b874-4048-b999-60b91291e7de",
   "metadata": {},
   "source": [
    "#### Evaluate the model\n",
    "\n",
    "Now let's evaluate the performance of the RDF Classifier on on the `X_aus` dataset. The accuracy should be around $42\\%$\n",
    "\n",
    "- This is the same dataset for the last notebook. If you want to review more results (such as full test data or confusion matrices), please review your previous notebook.\n",
    "\n",
    "We should also generate the predictions of the model for the dataset, and store them in `Y_aus_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0435a4-2a2d-4e32-9302-0099414ae121",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get model predictions\n",
    "print(f'{fer_rdf_model.score(X_aus, Y_aus_true) * 100:0.2f}% Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8995c7c5-d241-4c5f-b29c-123e4953022e",
   "metadata": {},
   "source": [
    "#### Generate model predictions for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9038369-38d3-41fa-ab4d-090e069c4824",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_aus_pred = fer_rdf_model.predict(X_aus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86151a98-9a7e-4fe2-9148-db8c3e577c35",
   "metadata": {},
   "source": [
    "### Review the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac46c812-4e56-4517-b906-8df76c0c7993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displays first 9 images in array\n",
    "start = 40\n",
    "\n",
    "# Gets all images from folder\n",
    "fau_images = [io.imread(f) for f in df_aus_xai.image]\n",
    "\n",
    "# gets labels for ground truth and predictions\n",
    "true_labels = [CLASS_LABELS[idx] for idx in Y_aus_true]\n",
    "pred_labels = [CLASS_LABELS[idx] for idx in Y_aus_pred]\n",
    "\n",
    "utils.display_nine_images(fau_images, true_labels, pred_labels, start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055926e2-cd9a-46cf-a3f2-69cab10b79e8",
   "metadata": {},
   "source": [
    "### Identify Some Images to Explain\n",
    "- Review the FAU dataset using the helper code in above.\n",
    "- Change the start value to get a new set of images (there are 10 images for each class, so for example, the class happy will be at indexes 10-19)\n",
    "- Search through the images to find at least 4 to explain \n",
    "    - Find classes that you would like to explain, and from each class select 2 images\n",
    "        - one should be a correct prediction  \n",
    "        - and one should be an incorrect prediction\n",
    "    - For each image, also choose the desired class index that you would like to generate counterfactuals for\n",
    "        - make sure to think about what is important for a desired class based on the prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7493fb6-c1f5-431b-a372-4481f4f70966",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Enter the Indexes Here ### \n",
    "###############################\n",
    "# you will use these arrays later in the task\n",
    "img_idxs = []\n",
    "desired_classes = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8ab172-667f-4477-985d-85ac631b40b9",
   "metadata": {},
   "source": [
    "## Task 1: Generate Explanations with DiCE\n",
    "In this part of the notebook, you will generate Counterfactual Explanations using the Python Library, [Diverse Counterfactual Explanations (DiCE)](http://interpret.ml/DiCE/). Make sure to read the documentation and getting started information.\n",
    "\n",
    "Counterfactual explanations typically work best on tabular data, so in this part we will are using the FAU dataset with the RDF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8547fad9-0a92-427c-9754-5fb71d8fe2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DiCE imports\n",
    "import dice_ml\n",
    "from dice_ml.utils import helpers  # helper functions\n",
    "\n",
    "pd.set_option('display.max_columns', None) # so that Jupyter doesn't truncate columns of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b556f3-fc0b-41df-9a5e-a01c08f735db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify categorical and numerical features as needed by DiCE\n",
    "categorical_features = list(df_aus_train.columns[df_aus_train.columns.str.contains('_c')])\n",
    "numerical_features = list(df_aus_train.columns[df_aus_train.columns.str.contains('_r')])\n",
    "\n",
    "# convert categorical features to strings as required by DICE\n",
    "df_aus_train[categorical_features] = df_aus_train[categorical_features].astype('str')\n",
    "df_aus_xai[categorical_features] = df_aus_xai[categorical_features].astype('str')\n",
    "df_aus_train[categorical_features] = df_aus_train[categorical_features].astype('category')\n",
    "df_aus_xai[categorical_features] = df_aus_xai[categorical_features].astype('category')\n",
    "\n",
    "all_features = numerical_features + categorical_features "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e70672-504f-4fa1-9f14-fcd2962a90e4",
   "metadata": {},
   "source": [
    "### Task 1.1 \n",
    "In this task, you will use DiCE to generate a set counterfactual explanations for your selected instances.\n",
    "\n",
    "#### Task 1.1.1 Setup a DiCE explainer instance\n",
    "\n",
    "See the [intro to DiCE](http://interpret.ml/DiCE/notebooks/DiCE_getting_started.html) for details on working with this library.\n",
    "\n",
    "Note: DiCE requires requires pandas dataframes for creating explainers and explanations. \n",
    "- for setting up the explainer you can use the following to create a dataframe of features and classes from the training data\n",
    "    - `df_aus_train[all_features+['class']`\n",
    "- for generating instances to explain, you can use the following code:\n",
    "    - `df_aus_xai[all_features][40:41]` where 40 is the index of instance to explan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151614f3-f9b3-4131-b012-0757f068f34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR CODE GOES HERE #####\n",
    "###############################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e418ac62-4d7e-4cf4-a027-ee49945f89e9",
   "metadata": {},
   "source": [
    "#### Task 1.1.2: Use the Explainer to Generate Counterfactual Explanations\n",
    "\n",
    "Generate counterfacutal explanations for each of your select data instances from task 3.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ce0a1b-7c44-4225-8d2c-6adb07943f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR CODE GOES HERE #####\n",
    "###############################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee342a5-bfb5-46f5-a8e2-00e194dc1ddb",
   "metadata": {},
   "source": [
    "#### Task 1.1.3: Visualize Counterfactuals\n",
    "\n",
    "Now visualize the counterfactuals as Pandas dataframes. \n",
    "\n",
    "It would also be helpful to include the original image with the explanation, as well as to print the label names for the ground truth, the prediction, and the desired outcome.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abc4d1b-0b0c-4d64-bc32-0f4e91d7c010",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR CODE GOES HERE #####\n",
    "###############################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4899f1d9-06a6-4288-bfdb-a0af37780c3a",
   "metadata": {},
   "source": [
    "#### Task 1.1.4: Describe your observations\n",
    "\n",
    "![Action Units](assets/fac.jpg)\n",
    "\n",
    "1. Which features are most important for the detection of the specific facials expressions of your data instances?  Do the counterfactuals make sense according to your intuition of the contrastive expression class you're using?\n",
    "2. The `generate_counterfactuals` method has a parameter `features_to_vary` so that we can restrict which features are perturbed in CF generation.  Are there any AUs that shouldn't be perturbed for our task of emotion detection? Why or why not? Additionally, should we set `permitted_range` parameter to limit the ranges of our continous features?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ab3b23-15cb-4a6a-b244-c38108123594",
   "metadata": {},
   "source": [
    "### Task 1.2 Generate Feature Attribution Scores from Counterfactuals\n",
    "\n",
    "DiCE can also generate [local and global feature attribution scores](http://interpret.ml/DiCE/notebooks/DiCE_getting_started.html#Generating-feature-attributions-(local-and-global)-using-DiCE) based on the identified counterfactuals.  In this task, we will do just that.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f727aed0-7465-4bc6-9fb0-e669b4fafd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting importance dictionaries provided by DiCE\n",
    "def plot_importance_dict(importance_dict):\n",
    "    keys = list(importance_dict.keys())\n",
    "    vals = [float(importance_dict[k]) for k in keys]\n",
    "    sns.barplot(x=keys, y=vals)\n",
    "    plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbfea3e-8579-47e5-9b86-eb426639a4dc",
   "metadata": {},
   "source": [
    "#### Task 1.2.1 Generate and Plot Local Importance Scores\n",
    "\n",
    "Using your previously defined DiCE explainer, generate and plot (with the help of the function above) local importance scores your your data instances.\n",
    "\n",
    "Again, it is helpful to also include the original image and FAU values, as well as to print the label names for the ground truth, the prediction, and the desired outcome.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c73f2a6-9d8d-4860-a702-a84d47bfe24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR CODE GOES HERE #####\n",
    "###############################\n",
    "from IPython.display import display # (use display to display a dataframe)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd7efff-419f-4d34-a6f3-f1ddb5920c72",
   "metadata": {},
   "source": [
    "#### Task 1.2.2 Generate and Plot Local Importance Scores\n",
    "\n",
    "Using your previously defined DiCE explainer, generate and plot global importance using the entire XAI dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeab7dd-266a-40e6-aeb0-21a714822fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR CODE GOES HERE #####\n",
    "###############################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a44a49-dafd-449f-8fef-a798d2d99337",
   "metadata": {},
   "source": [
    "#### Task 1.2.3 Describe your findings\n",
    "\n",
    "![Action Units](./assets/fac.jpg)\n",
    "\n",
    "1. Based on the DiCE documentation, how does DiCE calculate feature importance from counterfactuals?\n",
    "2. Do the plots lead to any interesting insights regarding AUs or facial expression detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf35c56-31b1-47bf-9573-f508b636d190",
   "metadata": {},
   "source": [
    "Write your answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b3139a-0b96-43cb-a49b-41e59438f6ad",
   "metadata": {},
   "source": [
    "## Task 2 - SHAP Explanations and Measuring Interpretability\n",
    "\n",
    "In this part of the Notebook, you will generate Shaply Value based saliency map explanations for predictions from the FER CNN from the last notebook. To do this, you will using the [SHAP Python Package](https://shap.readthedocs.io/en/latest/index.html), which is an extensive implementation of explantion methods using shapley values.  For this section, we will focus on just the CNN based explanation method.  \n",
    "\n",
    "We will also perform a \"sanity check\" of the SHAP output using the methodolgy we learned about in the [\"Sanity Checks for Saliency Maps\" paper](https://lernraumplus.uni-bielefeld.de/mod/folder/view.php?id=810572).  \n",
    "\n",
    "Before getting started with this task make sure to review the documention of SHAP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e213a43f-4450-4e62-82c3-0dbaebb77b04",
   "metadata": {},
   "source": [
    "### Load the Model and Data\n",
    "Here we are loading the pretrained Convolutional Neural Network for Facial Expression Recognition (FER) trained on raw images from a subset of the [AffectNet dataset](http://mohammadmahoor.com/affectnet/). \n",
    "\n",
    "This is the same model as our previous notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7290b787-c964-436f-b485-dcf37b4b09a3",
   "metadata": {},
   "source": [
    "#### Load the CNN model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf02890-29f8-4b49-a244-a0b20b1e0c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure you've downloaded the models from LernraumPlus (see README instructions for Notebook I)\n",
    "model_path = '../models/affectnet_model_e=60/affectnet_model'\n",
    "\n",
    "# test loading weights\n",
    "fer_cnn_model = cnn_model(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), num_classes=NUM_CLASSES)\n",
    "fer_cnn_model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a3268f-e3dd-437d-a8da-ca17e00e119f",
   "metadata": {},
   "source": [
    "#### Load the data\n",
    "`ImageDataGenerator` is a [Keras utility class](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) to easily load images for processing with a Keras model.\n",
    "\n",
    "The numpy array `X_img` contains 80 images that we will use for explanations.  And `Y_img_true` stores the ground truth labels, encoded as [one hot vectors](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/), for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26318c03-1852-4d46-8497-043669dda7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = '../data/affectnet/val_class/'\n",
    "# test_dir = '../localdata/affectnet/val_class/'\n",
    "\n",
    "\n",
    "# Load data\n",
    "test_datagen = ImageDataGenerator(validation_split=0.2,\n",
    "                                  rescale=1./255)\n",
    "test_gen = test_datagen.flow_from_directory(directory=test_dir,\n",
    "                                            target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            shuffle=False,\n",
    "                                            color_mode='rgb',\n",
    "                                            class_mode='categorical', \n",
    "                                            seed = SEED)\n",
    "X_img, Y_img_true = next(test_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec44a20-53df-47e5-ad64-6c0c6d334579",
   "metadata": {},
   "source": [
    "#### Evaluate model\n",
    "Next we will evaluate the loaded model to ensure it is working as expected.  You should get around $48.75\\%$ accuracy. While this is not a perfect classifier is well above random guessing which is $1 / 8 * 100 = 12.5$ accuracy\n",
    "\n",
    "This is the same CNN model as before, so refer to our previous notebook to view more details on its performance.\n",
    "\n",
    "We also generate the predictions of the model for the dataset, and store them in `Y_img_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0c6f3a-b682-4f8d-9c6d-6dcf83e29712",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = fer_cnn_model.evaluate(test_gen, verbose=2)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5672f5da-99c0-46be-a000-80136325ca01",
   "metadata": {},
   "source": [
    "#### Generate model predictions for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9562bbe-5f4b-4f54-81c9-e940130f8aa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Y_img_pred = fer_cnn_model.predict(X_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de80927e-a13b-4e93-bcfb-2f0f0ced1eb0",
   "metadata": {},
   "source": [
    "### Review the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd54dad-245d-478d-b2ea-913b9ee193fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displays first 9 images in array\n",
    "start = 0\n",
    "\n",
    "# gets labels for ground truth and predictions\n",
    "true_labels = [CLASS_LABELS[idx] for idx in np.argmax(Y_img_true, axis=1)]\n",
    "pred_labels = [CLASS_LABELS[idx] for idx in np.argmax(Y_img_pred, axis=1)]\n",
    "\n",
    "utils.display_nine_images(X_img, true_labels, pred_labels, start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4514f71-0288-4dd2-ad66-e94e62548c8a",
   "metadata": {},
   "source": [
    "### Identify Some Images to Explain\n",
    "- Review the FAU dataset using the helper code above.\n",
    "- Try changing start value to get a new set of images (there are 10 images for each class, so for example, the class happy will be at indexes 10-19)\n",
    "- Search through the images to find at least 4 to explain \n",
    "    - Find classes that you would like to explain, and from each class select 2 images\n",
    "        - one should be a correct prediction  \n",
    "        - and one should be an incorrect prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76924c9a-e9b2-41fb-8724-99a6e3dd8bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Enter the Indexes Here ### \n",
    "###############################\n",
    "# you will use this array later in the task\n",
    "img_idxs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2f1b7f-4b21-42f6-9707-84aaad22d04c",
   "metadata": {},
   "source": [
    "### Task 2.1: DeepLift based Explanations with SHAP\n",
    "\n",
    "In this task we will generate feature attribution explanations for our AffectNet CNN using SHAP and its implementation of an enhanced version of DeepLIFT, called [DeepExplainer](https://shap-lrjball.readthedocs.io/en/latest/generated/shap.DeepExplainer.html). To better understand the method you can review the [API documentation](https://shap-lrjball.readthedocs.io/en/latest/generated/shap.DeepExplainer.html).\n",
    "\n",
    "Unfortunatly, one of the downsides to this approach is computation time. If you're not running this notebook on a GPU you may have to be a bit patient when calculating the SHAP  values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f63e6a-9f9e-4103-9de9-e39a6875eeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we have to import shap and add a few fixes for our model\n",
    "import shap\n",
    "\n",
    "# fixes issues with running deep explainer on our model\n",
    "# https://github.com/slundberg/shap/issues/1761\n",
    "shap.explainers._deep.deep_tf.op_handlers[\"AddV2\"] = shap.explainers._deep.deep_tf.passthrough\n",
    "shap.explainers._deep.deep_tf.op_handlers[\"FusedBatchNormV3\"] = shap.explainers._deep.deep_tf.linearity_1d(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bae9836-2193-4a13-9864-eb65253e15e7",
   "metadata": {},
   "source": [
    "#### Task 2.1.1 Generate SHAP Values \n",
    "\n",
    "Generate the SHAP values for each of the four images you selected above.  You can review the [DeepExplainer Tutorial](https://shap-lrjball.readthedocs.io/en/latest/example_notebooks/deep_explainer/Front%20Page%20DeepExplainer%20MNIST%20Example.html) for help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae11a54-b312-44a0-8dfd-7ac0f48ab806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the shap deep explainer accepts a batch of input to generate \n",
    "# shap values for all inputs in one call the generate method \n",
    "images_xai = X_img[img_idxs]\n",
    "labels_true_xai = np.array(true_labels)[img_idxs]\n",
    "labels_pred_xai = np.array(pred_labels)[img_idxs]\n",
    "\n",
    "#### Your Code Here ####\n",
    "########################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90f33c5-775b-4380-88b4-b370594af80d",
   "metadata": {},
   "source": [
    "#### Tasks 2.1.2 Visualize the Explanations\n",
    "\n",
    "Now that we have calculated the SHAP values, lets visualize them. Use the SHAP API, from the above references, to plot the shap values as images.  Instead of printing the only the true label with the original image, print the true and predicted labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b00c72-6f25-4545-b963-159980ac31ff",
   "metadata": {},
   "source": [
    "#### Task 2.1.3 Evaluate SHAP Values\n",
    "One of the main features of SHAP values is that they sum up to the difference between the expected model output (i.e. the average of the predictions on the 'background sample') and the current output.\n",
    "\n",
    "In other words,\n",
    "\n",
    "$$\n",
    "   \\Sigma shap(x) = f(x) - E[f(X)]\n",
    "$$\n",
    "\n",
    "where $f(x)$ is our model, $x$ is the current instance being explained, and $X$ is the background sample (in our case the data in `X_xai`)\n",
    "\n",
    "Using the generated SHAP values and the generated model predictions, show that this is true for the Deep Explainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55aa656-9eab-484d-869d-ba44b5f58839",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Your Code Here ####\n",
    "########################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c52e75-dffa-4314-996b-955e0d3a2af7",
   "metadata": {},
   "source": [
    "### Task 2.2 Evaluate SHAPs Invariance to Model Changes\n",
    "\n",
    "In the paper \"Sanity Checks for Saliency Maps\" we saw the authors validated feature attribution methods by testing their invariance to models with randomly initialized weights (i.e. models that have not been trained). We will do exactly that in this task to evaluate SHAP's DeepExplainer method. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c10516-ef55-4a9d-b644-fae0ad1e7cf9",
   "metadata": {},
   "source": [
    "#### Task 2.2.1 Create Randomly Intialized Model\n",
    "\n",
    "First let's see what SHAP explanations for a model with completely randomized weights look like. \n",
    "\n",
    "This can be done by loading the CNN model without loading the saved weights (see the section on loading the data for code examples).  Once the model is loaded generate the predictions for our X_img dataset, as this will be needed to generate the SHAP explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af1877a-008d-4f65-bfc8-39428cd92836",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Your Code Here ####\n",
    "########################\n",
    "\n",
    "# replace None with the code to load the model\n",
    "fer_model_cnn_rnd = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f58796-9698-4079-b7b3-c89ae7584d9f",
   "metadata": {},
   "source": [
    "#### Task 2.2.2 Generate SHAP Values for Randomized Model\n",
    "\n",
    "Now generate the SHAP values for the random model and predictions, then visualize them as we did in tasks 2.1.1 and 2.1.2.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32423703-a913-46ef-91cb-4f74632ef4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Your Code Here ####\n",
    "########################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4221a6db-2ae2-468a-a78c-dc44c514239a",
   "metadata": {},
   "source": [
    "#### Task 2.2.3 Cascading Randomization\n",
    "\n",
    "Additionally in the paper, the authors evaluated the methods' invariance to randomized layers in a cascading fashion. In this task, we will also do that for SHAP.\n",
    "\n",
    "Using the code below as a template, generate SHAP values for a **single image**, from your selected images, as the weights of each layer are randomized.  Once you've generated SHAP values for each randomized layer, plot them similarly to the paper, including the name of the layer as the title for the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1a7533-8124-403d-aa31-a377e880bbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Randomized model for just the first image in your selection (for the sake of time)\n",
    "img_idx = 0\n",
    "img = X_img[img_idxs[img_idx]:img_idxs[img_idx]+1]\n",
    "\n",
    "# load a new model with pretrained weights\n",
    "fer_cnn_seqrnd = cnn_model(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), num_classes=NUM_CLASSES)\n",
    "fer_cnn_seqrnd.load_weights(model_path)\n",
    "\n",
    "# dictionary for storing randomized layer name and the shap values\n",
    "shap_vals_seq = {}\n",
    "\n",
    "#### Your Code Here ####\n",
    "########################\n",
    "# generate shap values for the original model (before randomizing) and save to the dictionary\n",
    "# only save the shap values for the predicted class\n",
    "\n",
    "# loop through the model layers in reverse and randomize each layer's weights as we go\n",
    "for i in range(1, len(fer_cnn_seqrnd.layers)):\n",
    "    if hasattr(fer_cnn_seqrnd.layers[-i], 'kernel_initializer'):\n",
    "        fer_cnn_seqrnd.layers[-i].set_weights(fer_model_cnn_rnd.layers[-i].weights) # copy weights from random model into current model\n",
    "        layer_name = fer_cnn_seqrnd.layers[-i].name\n",
    "        \n",
    "        #### Your Code Here ####\n",
    "        ########################\n",
    "        # generate shap values for each reinitialized layer and save to dictionary\n",
    "        # only save the shap values for the predicted class\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c6671a-8939-48ad-8337-bcfadf0ad968",
   "metadata": {},
   "source": [
    "##### Task 2.2.4\n",
    "\n",
    "Based on the above results would you consider SHAP invariant to model randomization? Why or why not? Furthermore, would you consider SHAP to be better than an Edge Detector?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b447d1-1a5e-473d-8c88-b820a29ca78c",
   "metadata": {},
   "source": [
    "### Bonus Task - Evaluate GradCAM for model invariance\n",
    "\n",
    "For a bonus task, perform the same evaluations of model invariance using our implementation of GradCAM from the last notebook (or any other feature map method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1e15a8-fe2f-40d5-9871-9b18a6745f56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
