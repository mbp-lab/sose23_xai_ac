{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59239cbb-a912-499c-b0f1-2fb9742217cb",
   "metadata": {},
   "source": [
    "# Explaining Facial Expression Recognition with Simplification and Feature Attribution\n",
    "# Notebook 1:  XAI for Affective Computing (SoSe2022)\n",
    "\n",
    "In this notebook you will attempt to generate explanations for predictions of two Facial Expression recognition models, one for tabular data extracted from images and one for raw image date, both trained using a subset of the [AffectNet dataset](http://mohammadmahoor.com/affectnet/). AffectNet is a dataset of facial expressions in the wild, and is labeled with 8 facial expression categories: **Neutral, Happy, Sad, Surprise, Fear, Disgust, Anger, and Contempt**. (Have a look at the paper for more details https://arxiv.org/abs/1708.03985). \n",
    "\n",
    "In **Part 1**, you will first explore local and global explanations on a tabular dataset of [Facial Action Units (FAUs)](https://imotions.com/blog/facial-action-coding-system/) by using the [LIME python package](https://github.com/marcotcr/lime). The dataset is comprised of FAUs that were extracted from the face images of AffectNet using [OpenFace2.0](https://github.com/TadasBaltrusaitis/OpenFace). This dataset is then used to train a Random Decision Forest (RDF) classifier (trained model is provided in the code below).\n",
    "\n",
    "In **Part 2**, you generate local explanations for a Convolutional Neural Network (CNN) using both LIME and GradCAM.  The CNN is already trained using the raw images of AffectNet (trained model is provided in the code below). A subset of the test data images is provided for generating and evaluating the explantions.  \n",
    "\n",
    "To use this notebook, please make sure to go step by step through each of the cells review the code and comments along the way.\n",
    "\n",
    "See **README** To get Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc2944a-a7fb-4d96-b652-8a2fe4f432d2",
   "metadata": {},
   "source": [
    "## Part 0: Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39040a80-0531-4da1-a98a-bd59f87d6cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295770a4-0087-4da4-80f6-15d5df6a18a9",
   "metadata": {},
   "source": [
    "**Import necessary libraries**\n",
    "\n",
    "(see README for necessary package installations if you receive a `module not found` error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b037e5c-d2f7-4c98-968b-e57231811d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "# import tensorflow for model loading\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# import sklearn for processing data and results\n",
    "from sklearn.metrics import confusion_matrix, classification_report, auc, roc_curve, roc_auc_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# import model loading function\n",
    "from model import cnn_model, create_bn_replacment\n",
    "\n",
    "# import plotting helper functions\n",
    "import utils\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfc15c5-cfb7-4daf-bff5-493b4c55fbaa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 1: Explanations of Facial Action Units\n",
    "\n",
    "In this part, we will generate explanations for the Random Decision Forest trained using a dataset of Facial Action Units (as described in the notebook introduction).  \n",
    "\n",
    "First, let's load the data and the trained models. Then we will evaluate the model peformance, before we start with the explanations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74272a30-eb14-4b45-80cb-01cc5f789639",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c592663b-e998-4208-b4e4-a638e1692b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full data from training and evaluation\n",
    "train_csv = '../data/affectnet_aus/train_aus.csv'\n",
    "val_csv = '../data/affectnet_aus/val_aus.csv'\n",
    "\n",
    "# load training and validation data as pandas dataframeas\n",
    "df_train = pd.read_csv(train_csv)\n",
    "df_val = pd.read_csv(val_csv)\n",
    "\n",
    "# smaller dataset for explanations (same data as in Task 1)\n",
    "xai_csv = '../data/affectnet_aus/eval_aus.csv'\n",
    "df_xai = pd.read_csv(xai_csv)\n",
    "\n",
    "# get only the columns storing action units from the dataframe\n",
    "feature_names = [col for col in df_val if col.startswith('AU')]\n",
    "categorical_features = [i for i, feat in enumerate(feature_names) if '_c' in feat]\n",
    "\n",
    "class_names = ['Neutral', 'Happy', 'Sad', 'Surprise', 'Fear', 'Disgust', 'Anger', 'Contempt']  # same class labels as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0b3bbf-7ae5-49ba-a2c3-0574d51e3da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data from dataframe to Numpy arrays\n",
    "X_train = np.array(df_train.loc[:, feature_names])\n",
    "y_train = np.array(df_train['class'])\n",
    "\n",
    "X_test = np.array(df_val.loc[:, feature_names])\n",
    "y_test = np.array(df_val['class'])\n",
    "\n",
    "X_xai = np.array(df_xai.loc[:, feature_names])\n",
    "y_xai = np.array(df_xai['class'])\n",
    "\n",
    "print('Train', X_train.shape, y_train.shape)\n",
    "print('Test', X_test.shape, y_test.shape)\n",
    "print('XAI', X_xai.shape, y_xai.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c5bf93-089c-4cfb-9d08-e07921c28bb7",
   "metadata": {},
   "source": [
    "### Load pretrained RDF model\n",
    "And validate that it works.  \n",
    "The accuracy of the model in the training data should be around $99.65\\%$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c262e012-07bb-41a2-be42-55fc5c8837dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../models/affect_rdf.pkl', 'rb') as f:\n",
    "    clf = pickle.load(f)\n",
    "    \n",
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080e8d94-975b-4750-bc14-e8316119d95a",
   "metadata": {},
   "source": [
    "### Now evaluate on the test data\n",
    "Unfortunately, the accuracy is only $44\\%$ but this is still well above chance guessing which would be $1 / 8 * 100 = 12.5\\%$ accuracy (since there are 8 total classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6619fa4-2606-4a5f-8ab2-ae377c72e74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model predictions\n",
    "y_test_preds = clf.predict(X_test)\n",
    "y_test_true = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a8d46d-dbf9-447c-9f18-f88d001142a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_true, y_test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def4d15c-2baa-4c2d-b0c6-8964feff96f9",
   "metadata": {},
   "source": [
    "We can also review the confusion matrix to see where the model makes its mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a018ad2b-4b76-4746-91bd-5226245e995a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cm_data = confusion_matrix(y_test_true, y_test_preds)\n",
    "cm = pd.DataFrame(cm_data, columns=class_names, index=class_names)\n",
    "cm.index.name = 'Actual'\n",
    "cm.columns.name = 'Predicted'\n",
    "plt.figure(figsize = (20,10))\n",
    "plt.title('Confusion Matrix', fontsize = 20)\n",
    "sns.set(font_scale=1.2)\n",
    "ax = sns.heatmap(cm, cbar=False, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16}, fmt='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87745914-96cb-41d3-a050-b4cf41261b09",
   "metadata": {},
   "source": [
    "### Evaluate with XAI Data\n",
    "Here we calculate and evaluate predictions on a smaller `X_xai` dataset.  `X_xai`is a subset of the full test data (from above) and will be used throughout the rest of part 1.\n",
    "\n",
    "The accuracy should be around $42\\%$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ff6cb2-b96a-43b8-9df6-5a2f77ab177f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get model predictions\n",
    "y_xai_preds = clf.predict(X_xai)\n",
    "y_xai_true = y_xai\n",
    "\n",
    "print(classification_report(y_xai_true, y_xai_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74afe66c-ff89-4e89-a98c-506c701fa7cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Display confusion matrix for `X_xai` dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9173f4e-8b76-4230-a84a-e8a5312773ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_data = confusion_matrix(y_xai_true, y_xai_preds)\n",
    "cm = pd.DataFrame(cm_data, columns=class_names, index=class_names)\n",
    "cm.index.name = 'Actual'\n",
    "cm.columns.name = 'Predicted'\n",
    "plt.figure(figsize = (20,10))\n",
    "plt.title('Confusion Matrix', fontsize = 20)\n",
    "sns.set(font_scale=1.2)\n",
    "ax = sns.heatmap(cm, cbar=False, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16}, fmt='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df582f5e-52e6-48b5-8fb8-6023b8b8bb51",
   "metadata": {},
   "source": [
    "### TASK 1: Implement LIME Local Explanations and SP-LIME for Global Explanations\n",
    "\n",
    "Now on to the implementation LIME explanations. \n",
    "\n",
    "#### Task 1.0: \n",
    "**Identify a Few Images to Explain**\n",
    "\n",
    "The code below will display images from the XAI dataset.\n",
    "- Try changing value of `start` to get a new set of images (there are 10 images for each class; for example, the class happy will be at indexes 10-19)\n",
    "- Search through the images to find at least 4 to explain \n",
    "    - Find classes that you would like to explain, and from each class select 2 images\n",
    "        - one should be a correct prediction  \n",
    "        - and one should be an incorrect prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d894a9d-7586-4f04-bc41-95f98b4e4713",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# packages needed for the rest of the tasks\n",
    "from skimage import io\n",
    "import lime.lime_tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e178e008-47b2-4134-936f-003776254d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets all images from folder\n",
    "images = [io.imread(f) for f in df_xai.image]\n",
    "\n",
    "# gets labels for ground truch and predictions\n",
    "true_labels = [class_names[idx] for idx in y_xai_true]\n",
    "pred_labels = [class_names[idx] for idx in y_xai_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda78971-e83c-447c-b04e-34521fd823fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displays first 9 images in array\n",
    "start = 10\n",
    "utils.display_nine_images(images, true_labels, pred_labels, start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62b8f3d-4fa5-4dde-95fd-960286ea9a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Enter the Indexes Here ### \n",
    "###############################\n",
    "# you will use this array later in the task\n",
    "img_idxs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7bf58a-aef0-42d8-953b-8a01673801a0",
   "metadata": {},
   "source": [
    "#### Task 1.1\n",
    "Now implement a [LimeTabularExplainer](https://lime-ml.readthedocs.io/en/latest/lime.html#module-lime.lime_tabular), you can review the [LIME tutorial](https://marcotcr.github.io/lime/tutorials/Tutorial%20-%20continuous%20and%20categorical%20features.html) for help. Make sure to indicate which feature are categorical features as stored in the `categorical_features` variable, as continuous and categorical features are treated differently with LIME.\n",
    "\n",
    "Note: In the feature names, you will see features with a `_c` and a `_r` at the end.  The `_r` means the intentsity of the action unit (i.e., how strong is it's presence), and the `_c` is a binary feature indicating the presence (value=1), or non-presence (value=0), of an action unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400d1e72-80d6-413d-b2a8-671827f8609e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### YOUR CODE GOES HERE #####\n",
    "###############################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a9539b-3db5-4fbd-8740-5dfbfa98c8db",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Task 1.2\n",
    "Generate LIME explanations the previously identified 4 data instances from the `X_xai` dataset, using the `LimeTabularExplainer` and then plotting the explanations for each data instance (see tutorial mentioned above).  \n",
    "\n",
    "HINT: Before showing an explanation, plot the image using `utils.display_one_image()` utility function.  Use `plt.show()` immediately after calling `utils.display_one_image()` to display the image before the explanation charts.\n",
    "\n",
    "Make sure to print out the **True** and **Predicted** labels for each instance.\n",
    "\n",
    "Try experimenting with different parameters for the explainer and explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f664685c-0294-46ee-84fb-9a39ad77fe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR CODE GOES HERE #####\n",
    "###############################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb60985-2d58-4573-a2fa-be9badbfc5a8",
   "metadata": {},
   "source": [
    "#### Talk 1.3\n",
    "Identify the important Facial Actions Units and compare with the images at [Facial Action Units](https://imotions.com/blog/facial-action-coding-system/).  What insights do these local explanations provide?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53e9192-1997-4b5c-bede-f0e954e77730",
   "metadata": {},
   "source": [
    "Write your answer here...\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6ba13f-8f14-4d8a-b687-8f8f3ae1dca7",
   "metadata": {},
   "source": [
    "#### Task 1.4\n",
    "\n",
    "Now implement [Submodular Pick](https://lime-ml.readthedocs.io/en/latest/lime.html#lime-submodular-pick-module) instance to generate global explanations and see if it provides you with a more global perspective of how the model makes decisions. You can review the [LIME tutorial](https://github.com/marcotcr/lime/blob/master/doc/notebooks/Submodular%20Pick%20examples.ipynb) for help.\n",
    "\n",
    "Try setting `num_exps_desired` to 16 to try to get 2 examples per class (although this isn't guaranteed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439113bd-01d2-4bcf-b500-3d205cc47181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for submodular pick\n",
    "from lime import submodular_pick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269a06c3-33cd-455a-b258-bc7fc0c63063",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR CODE GOES HERE #####\n",
    "###############################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91567957-ed63-4319-9589-200b53d21a24",
   "metadata": {},
   "source": [
    "#### Task 1.5\n",
    "Now plot the explantions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d3cab1-39b0-4885-9d3d-f2ab158d43cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### YOUR CODE GOES HERE #####\n",
    "###############################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebad0ac4-b398-42f7-83b8-c8c99f0f75fd",
   "metadata": {},
   "source": [
    "#### Bonus Task\n",
    "Generate a pandas dataframe of the explanations and explore the dataframe to gain more insight into the explanations, see the [LIME tutorial](https://github.com/marcotcr/lime/blob/master/doc/notebooks/Submodular%20Pick%20examples.ipynb) for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef927e80-4dd7-4002-95cd-b8c5fca9de13",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR CODE GOES HERE #####\n",
    "###############################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fce866-1d75-4176-90c8-f34a49d7d5bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Task 1.6\n",
    "\n",
    "How does LIME Submodular Pick select explanations for a global perspective on the model?\n",
    "\n",
    "Identify important AUs for each of the classes and compare with the images at [Facial Action Units](https://imotions.com/blog/facial-action-coding-system/).  What insights do these explanations provide? Do you now have a better understanding of how the model is working? If not, what is lacking using the LIME approach and/or what could be done differently?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aca0f78-0d6b-4eb3-96f8-ca2b165c2b61",
   "metadata": {},
   "source": [
    "Write your answer here\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde3666d-420d-4bf4-b933-fa54646f1d37",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 2:  Local Explations of Facial Expression Recognition with Images\n",
    "\n",
    "In this part, we will generate explanations for the CNN trained using facial images (as described in the notebook introduction).\n",
    "\n",
    "First, let's load the data and the trained models. Then we will evaluate the model peformance, before we start with the explanations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131c5a59-febe-476a-99d3-e7fdbd4e7ff5",
   "metadata": {},
   "source": [
    "Set some global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6fd431-7494-4db7-a6f9-c070e4be00fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 12\n",
    "IMG_HEIGHT = 128\n",
    "IMG_WIDTH = 128\n",
    "BATCH_SIZE = 80 \n",
    "NUM_CLASSES = 8\n",
    "CLASS_LABELS = ['Neutral', 'Happy', 'Sad', 'Surprise', 'Fear', 'Disgust', 'Anger', 'Contempt']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98a57a3-5065-4a20-a40b-9d60f30763a5",
   "metadata": {},
   "source": [
    "### Load Pretrained CNN Model and Setup Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe2f6c6-c918-405d-80a5-fb4ae74aa07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure you've downloaded the models from LernraumPlus (see README instructions for Notebook I)\n",
    "model_path = '../models/affectnet_model_e=60/affectnet_model'\n",
    "\n",
    "# test loading weights\n",
    "model_xai = cnn_model(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), num_classes=NUM_CLASSES)\n",
    "model_xai.load_weights(model_path).expect_partial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73340803-65f0-41e7-a800-de56bdec1ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = '../data/affectnet/val_class/'\n",
    "\n",
    "# Load data\n",
    "test_datagen = ImageDataGenerator(validation_split=0.2,\n",
    "                                  rescale=1./255)\n",
    "test_gen = test_datagen.flow_from_directory(directory=test_dir,\n",
    "                                            target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            shuffle=False,\n",
    "                                            color_mode='rgb',\n",
    "                                            class_mode='categorical', \n",
    "                                            seed = SEED)\n",
    "images, classes = next(test_gen) # since batch size is set to 80, this will load the entire test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652e29bf-9bbc-4eb3-8fd1-9d9f6c801830",
   "metadata": {},
   "source": [
    "### Evaluation and Predictions\n",
    "Here we evaluate the loaded model to ensure it is working as expected.  You should get around $55\\%$ accuracy. While this is not a perfect classifier is above random guessing which is $1 / 8 * 100 = 12.5$ accuracy\n",
    "\n",
    "Then we load predictions to use throughout the notebook. \n",
    "\n",
    "The predictions results can then be viewed with a confusion matrix to see where the model is confused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d5d926-6b4a-400b-bf82-4e47c8bdbcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model_xai.evaluate(test_gen, verbose=2)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6532b5-e0b6-4d8f-8227-2e4858763eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get softmax predictions from model\n",
    "preds = model_xai(images)\n",
    "\n",
    "# convert predictions to integers\n",
    "y_pred = np.argmax(preds, axis=-1)\n",
    "y_true = np.argmax(classes, axis=-1)\n",
    "\n",
    "# print detailed results\n",
    "print(classification_report(y_true, y_pred, target_names=CLASS_LABELS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58419a6d-6d1b-4e83-b1b7-6e1625bda5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also review the confusion matrix\n",
    "cm_data = confusion_matrix(y_true, y_pred)\n",
    "cm = pd.DataFrame(cm_data, columns=CLASS_LABELS, index = CLASS_LABELS)\n",
    "cm.index.name = 'Actual'\n",
    "cm.columns.name = 'Predicted'\n",
    "plt.figure(figsize = (20,10))\n",
    "plt.title('Confusion Matrix', fontsize = 20)\n",
    "sns.set(font_scale=1.2)\n",
    "ax = sns.heatmap(cm, cbar=False, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16}, fmt='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43ce90b-8453-4ade-a830-3b7402ab4534",
   "metadata": {},
   "source": [
    "### TASK 2: LIME Local Prediction Explanations\n",
    "\n",
    "Now that we have our model setup, we will review the images and predictions to identify a few data instances to explain.  \n",
    "\n",
    "#### Task 2.0\n",
    "\n",
    "Identify a Few Images to Explain\n",
    "\n",
    "The code below will display images from the XAI dataset.\n",
    "\n",
    "- Try changing start value to get a new set of images (there are 10 images for each class, so for example, the class happy will be at indexes 10-19)\n",
    "- Search through the images to find at least 4 to explain \n",
    "    - Find classes that you would like to explain, and from each class select 2 images\n",
    "        - one should be a correct prediction  \n",
    "        - and one should be an incorrect prediction\n",
    "        - These can be the same as previously select (but note that that the indexes are different due to different methods of reading the files from disk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd54dad-245d-478d-b2ea-913b9ee193fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displays first 9 images in array\n",
    "start = 30\n",
    "\n",
    "true_labels = [CLASS_LABELS[idx] for idx in y_true]\n",
    "pred_labels = [CLASS_LABELS[idx] for idx in y_pred]\n",
    "utils.display_nine_images(images, true_labels, pred_labels, start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02e8bf2-6d47-4051-8399-a7e04905e9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Enter the Indexes Here ### \n",
    "###############################\n",
    "# you will use this array later in this task\n",
    "img_idxs = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdbf361-07c1-457d-bb61-20589a6b25e7",
   "metadata": {},
   "source": [
    "#### Task 2.1 \n",
    "**Implement a LIME Image Explainer**\n",
    "\n",
    "Implement a [LimeImageExplainer](https://lime-ml.readthedocs.io/en/latest/lime.html#module-lime.lime_image) instance, you can review the [LIME tutorial](https://github.com/marcotcr/lime/blob/master/doc/notebooks/Tutorial%20-%20Image%20Classification%20Keras.ipynb) for help. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193f1575-52d8-4929-b2fd-55c8adec047b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "from lime import lime_image\n",
    "from lime.wrappers.scikit_image import SegmentationAlgorithm\n",
    "\n",
    "from skimage.segmentation import mark_boundaries # used to get boundries from explanation for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038b5151-4480-409b-b250-f62439867967",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR CODE GOES HERE #####\n",
    "###############################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875e5fe1-8515-410f-a697-623e077d759f",
   "metadata": {},
   "source": [
    "#### Task 2.2\n",
    "\n",
    "Now generate the explanations for each of the selected images.\n",
    "\n",
    "As we discussed in the seminar, LIME requires images to be segmented into superpixels.  For facial expressions the segmentation algorithm is very important and the default of the `explain_instance()` method may not provide good explanations.  Experiment with different segmenters using the LIME `SegmentationAlgorithm` class and pass to `segmenter_fn` argument of the `explain_instance()` method.\n",
    "\n",
    "- For example: `segmenter = SegmentationAlgorithm('method name', params)`, where params are defined by the skimage segmentation alorithm method\n",
    "- by default LIME uses: \n",
    "```\n",
    "segmenter = SegmentationAlgorithm('quickshift', kernel_size=4,\n",
    "                                  max_dist=200, ratio=0.2,\n",
    "                                  random_seed=random_seed)\n",
    "```\n",
    "\n",
    "You can find different algorithms via [skimage segmentation](https://scikit-image.org/docs/stable/api/skimage.segmentation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad57f8a-5b49-4fef-b406-bce666f255ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR CODE GOES HERE #####\n",
    "###############################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b2bf55-d8bb-4907-8140-208ea94e332a",
   "metadata": {},
   "source": [
    "#### Task 2.3\n",
    "Print the predicted labels for the top $N$ labels as found by explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f87a55d-a143-4485-a5d6-389ea7ef5ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR CODE GOES HERE #####\n",
    "###############################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ea942c-41c2-4e5a-912f-963d82240ed3",
   "metadata": {},
   "source": [
    "#### Task 2.4: \n",
    "\n",
    "**Visualize Explanations**\n",
    "\n",
    "Visualize the explanations for each of the 4 data points from LIME using matplotlib's `imshow` function (see above tutorial). (Or pass the explanation to the `display_one_image` from the `utils` module.)\n",
    "\n",
    "*HINT*: Use the `subplot` parameter of the `display_one_image` to plot a 2x2 grid.  The value should be an integer formated as `RCN` where `R` is the number of rows, `C` is the number of columns, and `C` is the number of the image to plot.  For example, `221` means to plot the first image of a 2x2 grid, `222` means the plot the second images, and so forth... (also see `display_nine_images` for example of this usage.)\n",
    "\n",
    "Experiment with at least 2 different sets of parameters for the explanation visualizations.  For example, view positive and negative contributions, change the number of features for the explation, or try visualizing a heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb71a9a8-bd68-4a54-b330-41d611ffd55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR CODE GOES HERE #####\n",
    "###############################\n",
    "# (you can use more than one notebook cell for this task)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5421706d-4b9a-4ed2-af6b-fec85dd15f84",
   "metadata": {},
   "source": [
    "#### Task 2.5 \n",
    "**Report on your findings**\n",
    "\n",
    "What are your insights have you gained about the predictions? Can you identify any patterns that explain how the model is working? Are you more or less confident in the model's performance after reviewing the explanations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f73f8b5-b207-46e6-bdce-e5614ad4871f",
   "metadata": {},
   "source": [
    "Write here findings here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a1d34e-1141-4f16-a0be-131a3d4f7a0c",
   "metadata": {},
   "source": [
    "### TASK 3: Grad CAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176599cf-57bd-415e-9fd2-053a66c00f04",
   "metadata": {},
   "source": [
    "#### Task 3.1\n",
    "**Implement the GradCAM Algorithm**\n",
    "\n",
    "Implement a version of GradCAM based on the [Keras GradCAM Tutorial](https://keras.io/examples/vision/grad_cam/). To better understand the algorithm, rather than just copying and pasting the entire functions, try implementing each main step of the alorithm in a seperate cell.  Then review the output of that cell either by printing the tensor or tensor shape.  Or by visualizing the output with matplotlib, for example with `plt.imshow()`\n",
    "\n",
    "note:  the final convolution layer in our network is named `final_conv_layer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196852c4-6664-40fb-af15-4ac18c0d70bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR CODE GOES HERE #####\n",
    "###############################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05be98ae-e7f6-4d7e-80d3-2faa0476baf9",
   "metadata": {},
   "source": [
    "#### Task 3.2\n",
    "\n",
    "Wrap the GradCAM Implementation into functions for generating the heatmaps and creating a superimposed image.  Instead of saving the superimposed image to a file (as done in the tutorial), simply generate the superimposed image and return it. We will use it in the next task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b176bf-0d1a-4c9f-99dc-f820d865cb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR CODE GOES HERE #####\n",
    "###############################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee759c8-6d3f-4d87-b8f2-dfbbfbd3eed5",
   "metadata": {},
   "source": [
    "#### Task 3.3\n",
    "\n",
    "Using your new GradCAM functions and the `display_one_image()` function in the `utils` model display the heatmaps for your images.  However, instead of just displaying the heatmaps for the top predicated class, for each of your images generate a heat map for each class label, see the file `gradcam_example.png` for an example of what this might look like. Also, for each image, add a border to the image representing the predicted class.  See the docstring of `display_one_image()` for details on usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a67f89-67bf-468d-a551-3c99fca35d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR CODE GOES HERE #####\n",
    "###############################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a322229-6d56-411d-a4e2-8a4fb6c36f23",
   "metadata": {},
   "source": [
    "#### Task 3.5\n",
    "\n",
    "What patterns do you notice in the heatmaps for the different classes. Do the regions of the image where the model focuses make sense? Or do you notice any unusual biases?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88389d57-9fac-423b-bdc7-50eede30dae4",
   "metadata": {},
   "source": [
    "### TASK 4: Final Discussion\n",
    "Between tasks 1, 2, and 3, which of the 2 methods best help you understand how the model is function, as we've discussed throughout the seminar. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0730cce4-3b0a-4709-836e-15fe01acebe5",
   "metadata": {},
   "source": [
    "write your answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448fed70-2112-406a-81ad-63e23c0d9d42",
   "metadata": {},
   "source": [
    "### BONUS TASK\n",
    "\n",
    "Try plot LIME output and GradCAM outpt for the same images next too each other.  Are the results similar or are there difference in indentified important regions?\n",
    "\n",
    "Or feel free to generate your own plots or visualizations to help understand the explanations and models better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f70429e-b6c9-40c7-8abd-3b9dbaa001e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR CODE GOES HERE #####\n",
    "###############################\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
